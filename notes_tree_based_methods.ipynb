{
 "metadata": {
  "name": "",
  "signature": "sha256:38e99cbbba2cfb0e8ae3d8fa83b2399589754a683e1c2867308b33f2d66d1675"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#<span style=\"background-color:#66FF99)\">Tree-Based Methods</span>\n",
      "\n",
      "###<span style=\"background-color:#FF99FF)\">Basics of Decision Trees</span>\n",
      "* Can be applied to both regression and classification problems\n",
      "\n",
      "###1. Regression Trees\n",
      "* Log a variable so its distribution has more of a typical bell-shape\n",
      "* More interpretable\n",
      "* Process of building a regression tree:\n",
      "    1. Divide the data into different non-overlapping regions\n",
      "    * Every observation that falls in a region, we make the same prediction (which is the mean of the response values for training observations in the region\n",
      "* How to divide into different regions?\n",
      "    * Can be any shape but we divide into rectangles and boxes for simplicity\n",
      "    * Goal is to find boxes which minimize the RSS\n",
      "    * Impossible to consider every partition, so we take top-down, greedy approach (recursive binary splitting)\n",
      "    * Top-down: Begins at the top and split into different branches\n",
      "    * Greedy: At each step, best split is made instead of looking ahead and pick 1 which might lead to better\n",
      "* Tree Pruning - build tree once the decrease in RSS due to each split exceeds a threshold\n",
      "* Good strategy would be to grow a very large tree, then prune it to obtain a subtree\n",
      "* What is the best way to prune a tree? Cost complexity pruning (weakest link pruning) -  index by nonnegative tuning parameter $\\alpha$\n",
      "* **How to build a regression tree?**\n",
      "    1. Use recursive binary splitting to grow a large tree. Stop when each terminal node has less than a no. of observations\n",
      "    * Apply cost complexity pruning - to get best subtrees as a function of $\\alpha$\n",
      "    * Use K-fold CV to choose $\\alpha$ which minimize the average error\n",
      "    * Return the subtree from step 2 which uses the chosen value of $\\alpha$\n",
      "     \n",
      "###2. Classification Trees\n",
      "* Predict observation response based on most commonly occuring class of training observations\n",
      "* Same process as regression trees\n",
      "* Cannot use RSS, we use:\n",
      "    1. Classification error rate: Fraction of training observations which do not belong to the most common class\n",
      "    * Gini index: measure of total variance across the K classes (takes on small value when proportion of wrongly classified is small)\n",
      "    * Cross-entropy: Similar to Gini Index, small when error is small\n",
      "* When building a classification tree, Gini or Cross-entropy used to evaluate the quality of particular split\n",
      "* A tree might lead to 2 yes, it just shows the certainty of a yes\n",
      "\n",
      "###3. Trees vs Linear Models\n",
      "* Which is better?\n",
      "    * If relationship between features and responses well-approximated by linear regression, use linear regression\n",
      "    * If highly non-linear and complex relationship, use decision trees\n",
      "    * Use CV to estimate the test error\n",
      "\n",
      "###4. Adv and Disadv of Trees\n",
      "1. Easy to explain to people\n",
      "* Closer to mirror human decision-making \n",
      "* Can be displayed graphically, easily interpretable\n",
      "* Can handle qualitative predictors without creating dummy variables\n",
      "* HOWEVER, trees do not have same predictive accuracy (BUT! aggregating many decision trees, we can improve the performance!)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###<span style=\"background-color:#FF99FF)\">Bagging, Random Forests, Boosting</span>\n",
      "* Used to construct more powerful prediction models\n",
      "1. Bagging\n",
      "    * Decision trees suffer from high variance\n",
      "    * Bootstrap Aggregating (Bagging) - procedure for reducing the variance\n",
      "    * Averaging a set of observations reduces variance\n",
      "    * Not practical if we do not have access to multiple training sets, therefore use bootstrap by taking repeated sames from 1 training set\n",
      "    * Construct B regression trees using B bootstrapped training sets, and average the predictions\n",
      "    * Applying to classification trees: Use majority vote (predict based on most commonly occuring class among B predictions)\n",
      "    * Straightforward way to estimate test error of a bagged model (without CV): Out-of-Bag Error Estimation\n",
      "    * Use variable importance measures to see which variable is most important (based on decrease in Gini index)\n",
      "* Random Forests\n",
      "    * Improvement over bagged trees by de-correlating the trees\n",
      "    * At each split, algorithm NOT allowed to consider a majority of the available predictors\n",
      "    * RF force each split to consider only a subset of the predictors (decorrelating)\n",
      "    * Main difference between RF and Bagging: Choice of predictor subset size M\n",
      "* Boosting\n",
      "    * Instead of fitting the data hard and potentially overfitting, boosting learns slowly\n",
      "    * Difference between boosting and random forests: in boosting, growth of a particular tree takes into account other trees which have been grown. Smaller trees can aid in interpretability"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}