{
 "metadata": {
  "name": "",
  "signature": "sha256:1530b6002e232648b4f4e9fb332dee1f53429462a8acc6c26064beee2b32c0be"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "# import pattern.web as web\n",
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "pd.set_option('display.width', 500)\n",
      "pd.set_option('display.max_columns', 100)\n",
      "pd.set_option('display.mpl_style', 'default')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#BeautifulSoup Documentation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "html_doc = \"\"\"\n",
      "<html><head><title>The Dormouse's story</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
      "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
      "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
      "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
      "and they lived at the bottom of a well.</p>\n",
      "\n",
      "<p class=\"story\">...</p>\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 148
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "soup = BeautifulSoup(html_doc)\n",
      "print(soup.prettify())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<html>\n",
        " <head>\n",
        "  <title>\n",
        "   The Dormouse's story\n",
        "  </title>\n",
        " </head>\n",
        " <body>\n",
        "  <p class=\"title\">\n",
        "   <b>\n",
        "    The Dormouse's story\n",
        "   </b>\n",
        "  </p>\n",
        "  <p class=\"story\">\n",
        "   Once upon a time there were three little sisters; and their names were\n",
        "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
        "    Elsie\n",
        "   </a>\n",
        "   ,\n",
        "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
        "    Lacie\n",
        "   </a>\n",
        "   and\n",
        "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
        "    Tillie\n",
        "   </a>\n",
        "   ;\n",
        "and they lived at the bottom of a well.\n",
        "  </p>\n",
        "  <p class=\"story\">\n",
        "   ...\n",
        "  </p>\n",
        " </body>\n",
        "</html>\n"
       ]
      }
     ],
     "prompt_number": 113
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print soup.title\n",
      "print soup.title.name\n",
      "print soup.title.string\n",
      "print soup.title.parent.name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<title>The Dormouse's story</title>\n",
        "title\n",
        "The Dormouse's story\n",
        "head\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print soup.p\n",
      "print soup.p['class']\n",
      "print soup.a\n",
      "print soup.find_all('a')\n",
      "print soup.find(id='link3')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "['title']\n",
        "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>\n",
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
        "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>\n"
       ]
      }
     ],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for link in soup.find_all('a'):\n",
      "    print(link.get('href'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://example.com/elsie\n",
        "http://example.com/lacie\n",
        "http://example.com/tillie\n"
       ]
      }
     ],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(soup.get_text())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The Dormouse's story\n",
        "\n",
        "The Dormouse's story\n",
        "Once upon a time there were three little sisters; and their names were\n",
        "Elsie,\n",
        "Lacie and\n",
        "Tillie;\n",
        "and they lived at the bottom of a well.\n",
        "...\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print soup.head\n",
      "print soup.title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<head><title>The Dormouse's story</title></head>\n",
        "<title>The Dormouse's story</title>\n"
       ]
      }
     ],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print soup.body.b\n",
      "print soup.find_all('a')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<b>The Dormouse's story</b>\n",
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "head_tag = soup.head\n",
      "print soup.head\n",
      "print soup.head.contents[0]\n",
      "print head_tag.contents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<head><title>The Dormouse's story</title></head>\n",
        "<title>The Dormouse's story</title>\n",
        "[<title>The Dormouse's story</title>]\n"
       ]
      }
     ],
     "prompt_number": 127
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_tag = soup.title\n",
      "print soup.title\n",
      "print soup.title.contents\n",
      "print soup.title.contents[0]\n",
      "print title_tag.contents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<title>The Dormouse's story</title>\n",
        "[u\"The Dormouse's story\"]\n",
        "The Dormouse's story\n",
        "[u\"The Dormouse's story\"]\n"
       ]
      }
     ],
     "prompt_number": 129
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print soup.title.contents[0]\n",
      "print soup.title.contents[0].string"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The Dormouse's story\n",
        "The Dormouse's story\n"
       ]
      }
     ],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for string in soup.stripped_strings:\n",
      "    print(repr(string))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "u\"The Dormouse's story\"\n",
        "u\"The Dormouse's story\"\n",
        "u'Once upon a time there were three little sisters; and their names were'\n",
        "u'Elsie'\n",
        "u','\n",
        "u'Lacie'\n",
        "u'and'\n",
        "u'Tillie'\n",
        "u';\\nand they lived at the bottom of a well.'\n",
        "u'...'\n"
       ]
      }
     ],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "title_tag = soup.title\n",
      "print title_tag.contents[0]\n",
      "print title_tag.parent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The Dormouse's story\n",
        "<head><title>The Dormouse's story</title></head>\n"
       ]
      }
     ],
     "prompt_number": 144
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sibling_soup = BeautifulSoup(\"<a><b>text1</b><c>text2</c></b></a>\")\n",
      "print(sibling_soup.prettify())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<html>\n",
        " <body>\n",
        "  <a>\n",
        "   <b>\n",
        "    text1\n",
        "   </b>\n",
        "   <c>\n",
        "    text2\n",
        "   </c>\n",
        "  </a>\n",
        " </body>\n",
        "</html>\n"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sibling_soup.b.next_sibling\n",
      "print sibling_soup.c.previous_sibling"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<c>text2</c>\n",
        "<b>text1</b>\n"
       ]
      }
     ],
     "prompt_number": 147
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Searching the Tree - find() and find_all()\n",
      "Filtering using\n",
      "* tag's name\n",
      "* attributes\n",
      "* text of a string\n",
      "* combination of all above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "html_doc = \"\"\"\n",
      "<html><head><title>The Dormouse's story</title></head>\n",
      "<body>\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
      "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
      "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
      "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
      "and they lived at the bottom of a well.</p>\n",
      "\n",
      "<p class=\"story\">...</p>\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print soup.find_all('b')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[<b>The Dormouse's story</b>]\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find tags that start with b with regex\n",
      "import re\n",
      "for tag in soup.find_all(re.compile(\"^b\")):\n",
      "    print(tag.name)\n",
      "#find tags that start with t with regex\n",
      "for tag in soup.find_all(re.compile(\"t\")):\n",
      "    print(tag.name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "body\n",
        "b\n",
        "html\n",
        "title\n"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find all that match ANY item in the list\n",
      "soup.find_all([\"a\", \"title\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 157,
       "text": [
        "[<title>The Dormouse's story</title>,\n",
        " <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
        " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
        " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#find all the tags\n",
      "for tag in soup.find_all(True):\n",
      "    print(tag.name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "html\n",
        "head\n",
        "title\n",
        "body\n",
        "p\n",
        "b\n",
        "p\n",
        "a\n",
        "a\n",
        "a\n",
        "p\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Using a function to filter\n",
      "def has_class_but_no_id(tag):\n",
      "    return tag.has_attr('class') and tag.has_attr('id')\n",
      "\n",
      "soup.find_all(has_class_but_no_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 165,
       "text": [
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
        " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
        " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print soup.find_all('title')\n",
      "print soup.find_all('p', 'title')\n",
      "print soup.find_all(id='link2')\n",
      "print soup.find_all(id=True)\n",
      "print soup.find_all(href=re.compile(\"elsie\"))\n",
      "print soup.find_all(href=re.compile(\"elsie\"), id='link1')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[<title>The Dormouse's story</title>]\n",
        "[<p class=\"title\"><b>The Dormouse's story</b></p>]\n",
        "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n",
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n",
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]\n"
       ]
      }
     ],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print soup.find_all(\"a\", class_=\"sister\")\n",
      "print soup.find_all(text=\"Elsie\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
        "[u'Elsie']\n"
       ]
      }
     ],
     "prompt_number": 182
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "first_link = soup.a\n",
      "first_link.find_all_next('a')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 190,
       "text": [
        "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
        " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
       ]
      }
     ],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#CSS selectors\n",
      "print soup.select('title')\n",
      "print soup.select('body a')\n",
      "print soup.select('p > a')\n",
      "\n",
      "#select by CSS class\n",
      "print soup.select(\".sister\")\n",
      "#select by CSS ID\n",
      "print soup.select(\"#link2\")\n",
      "print soup.select('a[href]')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[<title>The Dormouse's story</title>]\n",
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
        "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>, <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>, <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
        "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n"
       ]
      }
     ],
     "prompt_number": 198
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Reddit"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Example links on the site\n",
      "-------------------------\n",
      "http://www.reddit.com/r/pics/comments/2p1tr8/my_dads_a_trash_truck_driver_he_got_this_today/\n",
      "http://www.reddit.com/r/AskReddit/comments/2p28u1/serious_people_who_went_missing_what_happened/\n",
      "\"\"\"    \n",
      "\n",
      "import requests\n",
      "from pattern import web\n",
      "from fnmatch import fnmatch\n",
      "\n",
      "def is_comments_page(l):\n",
      "    \"\"\"Returns true if link is comments page\"\"\"\n",
      "    pattern = 'http://www.reddit.com/r/*/comments/*/*/'\n",
      "    return fnmatch(l, pattern)\n",
      "\n",
      "def find_comments_pages(html):\n",
      "    dom = web.Element(html)\n",
      "    links = [a.attributes.get('href', '') for a in dom.by_tag('a')]\n",
      "    links = [l for l in links if is_comments_page(l)] #remove duplicates\n",
      "    links = list(set(links))\n",
      "    return links\n",
      "\n",
      "page = requests.get('http://www.reddit.com/').text.encode('ascii', 'ignore')\n",
      "\n",
      "find_comments_pages(page)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "[u'http://www.reddit.com/r/Jokes/comments/2p2xub/i_like_my_coffee_like_i_like_my_men/',\n",
        " u'http://www.reddit.com/r/aww/comments/2p1g0d/i_think_i_was_chosen_at_the_humane_society_today/',\n",
        " u'http://www.reddit.com/r/space/comments/2p2hsk/a_great_photo_of_the_milky_way_and_its_reflection/',\n",
        " u'http://www.reddit.com/r/WritingPrompts/comments/2p2xzf/wp_everyones_talking_about_that_crazy_thing_you/',\n",
        " u'http://www.reddit.com/r/science/comments/2p0a7u/morning_work_start_times_should_be_delayed_to/',\n",
        " u'http://www.reddit.com/r/worldnews/comments/2p13tc/isis_releases_horrifying_sex_slave_pamphlet/',\n",
        " u'http://www.reddit.com/r/food/comments/2p16gz/grilled_cheese_on_12grain_wheat_bread_to_dip_in/',\n",
        " u'http://www.reddit.com/r/Fitness/comments/2p1e77/can_exercise_help_my_depression/',\n",
        " u'http://www.reddit.com/r/Showerthoughts/comments/2p0xf3/every_year_reddit_should_take_that_years_top_12/',\n",
        " u'http://www.reddit.com/r/AskReddit/comments/2p2y1t/reddit_what_is_your_favorite_vine_or_short/',\n",
        " u'http://www.reddit.com/r/LifeProTips/comments/2p1cqm/lpt_think_very_carefully_before_establishing/',\n",
        " u'http://www.reddit.com/r/IAmA/comments/2p0yez/warren_g_regulators_mount_up/',\n",
        " u'http://www.reddit.com/r/funny/comments/2p20yq/how_i_returned_my_sisters_computer_after_fixing_it/',\n",
        " u'http://www.reddit.com/r/gaming/comments/2p13x3/piece_of_heart_heart_container_jewels_fan_art/',\n",
        " u'http://www.reddit.com/r/videos/comments/2p1yet/laser_mounted_to_dogs_collar_hilarity_ensues_103/',\n",
        " u'http://www.reddit.com/r/Futurology/comments/2p2pio/this_week_in_technology_an_advanced_laser_defense/',\n",
        " u'http://www.reddit.com/r/mildlyinteresting/comments/2p0nax/this_toothbrush_has_no_bristles/',\n",
        " u'http://www.reddit.com/r/history/comments/2ozuwe/a_crew_is_at_work_in_boston_unearthing_a_time/',\n",
        " u'http://www.reddit.com/r/todayilearned/comments/2p1ux2/til_jon_heder_was_only_paid_1000_for_his_role_as/',\n",
        " u'http://www.reddit.com/r/funny/comments/2p2y2s/mrw_trying_to_find_the_clit/',\n",
        " u'http://www.reddit.com/r/explainlikeimfive/comments/2p2km9/eli5_why_is_there_15_250mbs_internet_in_romania/',\n",
        " u'http://www.reddit.com/r/Jokes/comments/2p20v8/newfie_logic/',\n",
        " u'http://www.reddit.com/r/movies/comments/2p0ub2/star_wars_the_force_awakens_character_names/',\n",
        " u'http://www.reddit.com/r/news/comments/2p2xv4/studentsex_charges_dismissed_against_exassistant/',\n",
        " u'http://www.reddit.com/r/OldSchoolCool/comments/2p2y49/beautiful_courtney_cox_in_1988_photo_by_karen/',\n",
        " u'http://www.reddit.com/r/gifs/comments/2p2o77/stephen_colbert_interviews_smaug/',\n",
        " u'http://www.reddit.com/r/UpliftingNews/comments/2p2h2i/australian_police_arrest_man_just_25_minutes/',\n",
        " u'http://www.reddit.com/r/AskReddit/comments/2p28u1/serious_people_who_went_missing_what_happened/',\n",
        " u'http://www.reddit.com/r/pics/comments/2p1tr8/my_dads_a_trash_truck_driver_he_got_this_today/',\n",
        " u'http://www.reddit.com/r/InternetIsBeautiful/comments/2p2gfe/a_minimalist_beautiful_and_fun_soundbox_patapap/',\n",
        " u'http://www.reddit.com/r/food/comments/2p2xlk/im_chris_nuttallsmith_toronto_restaurant_critic/',\n",
        " u'http://www.reddit.com/r/news/comments/2p1fmq/nasa_gets_2_boost_to_science_budget/',\n",
        " u'http://www.reddit.com/r/Jokes/comments/2p2xvc/yo_momma_joke/',\n",
        " u'http://www.reddit.com/r/Showerthoughts/comments/2p2xi0/if_all_my_fantasies_and_wishes_came_true_the/',\n",
        " u'http://www.reddit.com/r/todayilearned/comments/2p2xxx/til_billy_bob_thornton_has_a_phobia_of_antique/']"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_links_from_front_pages(n):\n",
      "    'find  URLs of comments pages, linked from the n first few pages of reddit'\n",
      "    url = web.URL('http://www.reddit.com/')\n",
      "    comment_pages = []\n",
      "    for page_idx in range(n):\n",
      "        dom = web.DOM(url.download(cached=False))\n",
      "    \n",
      "        for entry in dom('a.comments'):\n",
      "            href = entry.attributes.get('href', '')\n",
      "            if href:\n",
      "                comment_pages.append(href)\n",
      "                \n",
      "        # find the next page link - reddit has 25 links per page\n",
      "        for a in dom('a'):\n",
      "            if ('count=%d' % ((page_idx + 1) * 25)) in a.attributes.get('href', ''):\n",
      "                url = web.URL(a.attributes.get('href'))\n",
      "    # use set() to remove repeated pages\n",
      "    return list(set(comment_pages))\n",
      "\n",
      "            \n",
      "print len(get_links_from_front_pages(6))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "172\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def info_from_comments_pages(links):\n",
      "    'fetch title, upvotes, downvotes, time of submission from a sequence of links'\n",
      "    results = []\n",
      "    for urltext in links:\n",
      "        url = web.URL(urltext)\n",
      "        print \"fetching info for\", url\n",
      "        try:\n",
      "            dom = web.DOM(url.download(cached=False))\n",
      "            title = dom('title')[0].content\n",
      "            upvotes = int(dom.by_class('upvotes')[0].children[0].content.replace(',', ''))\n",
      "            downvotes = int(dom.by_class('downvotes')[0].children[0].content.replace(',', ''))\n",
      "            time = dom.by_class('tagline')[0]('time')[0].attributes.get('datetime')\n",
      "            results.append((title, upvotes, downvotes, pd.to_datetime(time)))\n",
      "        except KeyboardInterrupt:\n",
      "            # allow us to interrupt the kernel but use what we've already fetched\n",
      "            break\n",
      "        except:\n",
      "            pass  # some things that look like comment pages don't have the information above\n",
      "    return results"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comments_pages = get_links_from_front_pages(1)\n",
      "print \"Fetching info for\", len(comments_pages), \"pages\"\n",
      "pages = info_from_comments_pages(comments_pages)\n",
      "titles, upvotes, downvotes, dates = zip(*pages)  # zip(*seq) transposes a sequence of sequences.\n",
      "df = pd.DataFrame({'title' : titles, 'upvotes' : upvotes, 'downvotes' : downvotes, 'date' : dates}, index=dates)\n",
      "print df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fetching info for 35 pages\n",
        "fetching info for http://www.reddit.com/r/LifeProTips/comments/2oy94v/lpt_you_can_access_all_the_piratebay_content/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/space/comments/2oyxbe/this_shot_of_typhoon_haiyan_from_a_japanese/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/explainlikeimfive/comments/2ox3yr/eli5_how_do_poops_and_farts_work_together_in_the/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/television/comments/2oxuq0/more_leaked_sony_emails_just_after_community_was/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/pics/comments/2oy1ew/cards_against_humanity_black_friday_bullshit/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/movies/comments/2oxjbt/eerie_poster_for_in_the_heart_of_the_sea/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/DIY/comments/2owae8/heres_a_tabletop_i_made_from_a_door_and_5218/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/funny/comments/2oyaar/back_to_the_fuchsia/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/Showerthoughts/comments/2oz18h/your_stomach_is_always_filled_with_warm_vomit/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/gifs/comments/2oz1sp/william_shatner_vs_piranha/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/photoshopbattles/comments/2ox53c/psbattle_pig_pushing_a_trolley_with_a_pint_of_beer/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/videos/comments/2oxjg7/yup_jetpacks_are_real_the_future_is_here/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/books/comments/2oylvs/my_friend_just_tweeted_ever_realised_how_fucking/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/mildlyinteresting/comments/2owxfz/bratman_a_portable_one_man_bratwurst_grilling/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/Jokes/comments/2oxcs4/when_i_was_five_my_dad_put_snowballs_in_the/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/AskReddit/comments/2oz1t1/could_anyone_identify_these_symbols_for_me_unsure/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/gaming/comments/2oxh3r/til_my_xbone_controller_is_different/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/worldnews/comments/2oz1q4/rectal_rehydration_and_standing_on_broken_limbs/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/science/comments/2oxl08/researchers_from_north_carolina_state_university/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/todayilearned/comments/2oxck4/til_mobile_users_in_poor_countries_can_access/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/AskReddit/comments/2oxel5/children_of_porn_stars_how_has_this_affected_you/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/food/comments/2oxfog/do_you_wanna_build_a_bento_box/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/IAmA/comments/2oxk19/nick_offerman_chanticleer_ready_for_another_ama/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/funny/comments/2oz1xm/christopher_lee_sings_heavy_metal_christmas_songs/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/AskReddit/comments/2oz212/what_is_your_reallife_glitch_in_the_matrix_moment/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/funny/comments/2oz1uu/william_shatner_vs_piranha/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/worldnews/comments/2ox2il/pope_to_shoppers_dont_buy_products_made_by/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/gifs/comments/2oy7x5/the_worlds_smallest_3d_printer_is_a_pen_that/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/movies/comments/2oz1sl/2015_golden_globe_nominations/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/Futurology/comments/2oybn5/superbugs_to_kill_more_than_cancer_by_2050/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/news/comments/2owgol/an_anonymous_wikipedia_user_from_an_ip_address/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/AskReddit/comments/2oz1sr/reddit_what_do_you_think_was_the_most_significant/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " r/trendingsubreddits/comments/2oyfgk/trending_subreddits_for_20141211/\n",
        "fetching info for http://www.reddit.com/r/aww/comments/2oxjah/my_friend_and_his_cat_nikita_on_her_birthday/\n",
        "fetching info for"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " http://www.reddit.com/r/tifu/comments/2oys4k/tifu_by_killing_my_puppy/\n"
       ]
      },
      {
       "ename": "ValueError",
       "evalue": "need more than 0 values to unpack",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-31-fcc681e33892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Fetching info for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pages\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo_from_comments_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomments_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupvotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownvotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# zip(*seq) transposes a sequence of sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtitles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'upvotes'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mupvotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'downvotes'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdownvotes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: need more than 0 values to unpack"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.sort('date', inplace=True)\n",
      "df['upvotes'].plot(c='g')\n",
      "df['downvotes'].plot(c='r')\n",
      "(df['upvotes'] - df['downvotes']).plot(c='k')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'df' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-30-33fa8ccf52f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'upvotes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'downvotes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'upvotes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'downvotes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import urllib2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://www.reddit.com'\n",
      "r = requests.get(url)\n",
      "print r\n",
      "soup = BeautifulSoup(r.content)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<Response [200]>\n"
       ]
      }
     ],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for link in soup.find_all('a', {'class':'comments'}):\n",
      "    print(link.get('href'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "http://www.reddit.com/r/Showerthoughts/comments/2p1re5/guaranteed_way_to_make_money_at_the_casino/\n",
        "http://www.reddit.com/r/Showerthoughts/comments/2p1s3m/netflix_should_have_a_good_to_watch_while_your/\n",
        "http://www.reddit.com/r/nottheonion/comments/2p1s50/wisconsin_man_blames_beer_battered_fish_on_10th/\n",
        "http://www.reddit.com/r/funny/comments/2p1s4y/atleast_she_is_honest/\n",
        "http://www.reddit.com/r/Art/comments/2p1s23/portrait_of_billy_mays_never_forget/\n",
        "http://www.reddit.com/r/singapore/comments/2p1n1u/sentosa_beaches_on_the_11th_spot_for_the_16_most/\n",
        "http://www.reddit.com/r/Showerthoughts/comments/2p1ro6/if_and_only_if_aliens_exist_then_we_are_aliens_too/\n",
        "http://www.reddit.com/r/gaming/comments/2p1run/thrift_store_gold_happy_birthday_ps1/\n",
        "http://www.reddit.com/r/AskReddit/comments/2p1s30/what_do_your_parents_do_for_work/\n",
        "http://www.reddit.com/r/videos/comments/2p1s0b/in_the_spirit_of_christmas/\n",
        "/r/trendingsubreddits/comments/2oyfgk/trending_subreddits_for_20141211/\n",
        "http://www.reddit.com/r/pics/comments/2p198q/undercover_cop_points_gun_at_protestors_after/\n",
        "http://www.reddit.com/r/Showerthoughts/comments/2p0xf3/every_year_reddit_should_take_that_years_top_12/\n",
        "http://www.reddit.com/r/funny/comments/2p134z/one_seriously_cordial_motherfucker/\n",
        "http://www.reddit.com/r/gifs/comments/2p0quj/my_friend_and_i_recently_made_an_iron_man_parody/\n",
        "http://www.reddit.com/r/gaming/comments/2p0g0o/intelligence_under_3_is_great/\n",
        "http://www.reddit.com/r/IAmA/comments/2p0yez/warren_g_regulators_mount_up/\n",
        "http://www.reddit.com/r/todayilearned/comments/2p031z/til_george_lucas_gave_mel_brooks_his_blessing_to/\n",
        "http://www.reddit.com/r/science/comments/2p0a7u/morning_work_start_times_should_be_delayed_to/\n",
        "http://www.reddit.com/r/movies/comments/2p0ub2/star_wars_the_force_awakens_character_names/\n",
        "http://www.reddit.com/r/news/comments/2ozwmi/cops_use_taser_on_woman_while_she_recorded_arrest/\n",
        "http://www.reddit.com/r/OldSchoolCool/comments/2p0o6d/beautiful_hippie_girl_selling_flowers_1972/\n",
        "http://www.reddit.com/r/worldnews/comments/2ozh22/canada_just_made_it_legal_for_police_to_search/\n",
        "http://www.reddit.com/r/AskReddit/comments/2ozsm8/reddit_what_rule_do_you_live_by/\n",
        "http://www.reddit.com/r/videos/comments/2p0n4b/nasas_parody_of_all_about_that_bass_all_about/\n",
        "http://www.reddit.com/r/history/comments/2ozuwe/a_crew_is_at_work_in_boston_unearthing_a_time/\n",
        "http://www.reddit.com/r/EarthPorn/comments/2ozttf/winter_lace_newark_ohio_1618x1080oc/\n",
        "http://www.reddit.com/r/mildlyinteresting/comments/2p0nax/this_toothbrush_has_no_bristles/\n",
        "http://www.reddit.com/r/photoshopbattles/comments/2ozhgq/psbattle_this_fish_with_grandpas_dentures/\n",
        "http://www.reddit.com/r/DIY/comments/2ozwb8/copper_flowers_just_in_time_for_the_holidays/\n",
        "http://www.reddit.com/r/space/comments/2ozj2b/the_mission_patch_for_tonights_final_ula_launch/\n",
        "http://www.reddit.com/r/Music/comments/2oznyb/john_lydon_says_sex_pistols_are_too_fat_to_reunite/\n",
        "http://www.reddit.com/r/sports/comments/2ozo0r/jason_williams_elbow_pass/\n",
        "http://www.reddit.com/r/creepy/comments/2oznks/view_of_the_moon_from_inside_a_cave/\n",
        "http://www.reddit.com/r/askscience/comments/2ozkcv/whats_the_point_of_linear_algebra/\n",
        "http://www.reddit.com/r/Futurology/comments/2oz1o3/iowa_to_allow_citizens_to_use_a_smartphone_app_as/\n"
       ]
      }
     ],
     "prompt_number": 102
    }
   ],
   "metadata": {}
  }
 ]
}