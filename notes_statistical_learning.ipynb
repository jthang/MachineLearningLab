{
 "metadata": {
  "name": "",
  "signature": "sha256:acd8806d763c14a7c4d806c1b4e55c7242139e0b2a53d6d2098612e528af91cb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Overview of Statistical Learning\n",
      "* Set of tools for understanding data\n",
      "* Tools classified as supervised or unsupervised\n",
      "* Build a model for predicting an output based on 1 or more inputs (supervised)\n",
      "* Inputs with no supervising output (unsupervised)\n",
      "\n",
      "###What is Statistical Learning?\n",
      "$Y = f(X) + \\epsilon$\n",
      "* A set of approaches to estimate $f$ (function) to get an output variable (response)\n",
      "\n",
      "###Why estimate $f$?\n",
      "* Prediction - to predict a response based on inputs\n",
      "    * Accuracy of response depends on:\n",
      "        1. reducible error - can be improved by using appropriate models\n",
      "        2. irreducible error - error which cannot be predicted\n",
      "* Inference - to understand the relationship of how response is affected by the inputs (predictors)\n",
      "    1. Which predictors are associated with the response?\n",
      "    2. What is the relationship between the response and each predictor?\n",
      "    3. Is the relationship linear or non-linear?\n",
      "\n",
      "###How to estimate $f$?\n",
      "* Parametric Methods - estimating a set of parameters\n",
      "* Non-parametric Methods - no explicit assumptions about the functional form of $f$\n",
      "\n",
      "###Trade-off between Prediction Accuracy and Model Interpretability\n",
      "* Should we choose a restrictive or flexible approach? When we just want inference, a restrictive (linear regression) is a good choice beacuse of it is more interpretable. When we want prediction and interpretability is not so important, we can choose a more flexible model. However, flexible model might lead to overfitting, which reduces accuracy.\n",
      " \n",
      "###Supervised vs Unsupervised Learning\n",
      "* Aim is to accurately predict the response for future observation (prediction) or better understand the relationship between response and predictors (inference)\n",
      "* Supervised learning: for each observation, there's an associated response.\n",
      "\n",
      "###Regression vs Classification\n",
      "* Quantitative response - regression\n",
      "* Qualitative response - classification"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Assessing Model Accuracy\n",
      "* No 1 method dominates all, so we need to find the best model\n",
      "* Finding the best model is one of the most challenging parts\n",
      "\n",
      "###Measuring Quality of Fit\n",
      "* Mean Squared Error (MSE)\n",
      "\n",
      "$MSE = \\frac{1}{n}\\sum_{i=1}^n(y_i - \\hat f(x_i))^2$\n",
      "\n",
      "MSE = Square of the error divide by no. of observations\n",
      "\n",
      "* $\\hat f(x_i)$: prediction that $\\hat f$ gives for the $i$th observation\n",
      "* MSE will be small if predicted responses close to true reponses. \n",
      "* Low Train MSE $\\neq$ Low Test MSE. \n",
      "* Flexibility $\\uparrow \\neq$ MSE $\\downarrow$ due to be overfitting the data\n",
      "* HOWEVER, we are more interested in selecting a method which minimize the **test MSE**. What if there are no test observations? Cross-validation.\n",
      "\n",
      "###Bias-Variance Trade Off\n",
      "* U-shaped test MSE result of variance and bias\n",
      "\n",
      "$E(y_0 - \\hat f(x_0))^2 = Var(\\hat f(x_0)) + [Bias(\\hat f(x_0))]^2 + Var(\\epsilon)$\n",
      "\n",
      "* Expected test MSE because of variance, squared bias, and variance of error term ($\\epsilon$)\n",
      "* Need to pick the one with the low variance and low bias\n",
      "* Variance = amount of $\\hat f$ would change if we change data\n",
      "    * More flexible = High Variance because fit the data closely\n",
      "* Bias = error caused by picking a too simple model\n",
      "    * Linear regression is restrictive, result high bias\n",
      "* Test MSE is U-shaped because initially bias decrease, but increase due to variance\n",
      "* Challenge: Find a method which variance and squared bias are low.\n",
      "\n",
      "### Classification Setting\n",
      "* Do not use MSE but use training error rate\n",
      "\n",
      "$\\frac{1}{n}\\sum_{i=1}I(y_i\\neq\\hat y_i)$\n",
      "\n",
      "* Traing error rate = sum of mistakes divide by number of observations. Good classifier = low test error\n",
      "\n",
      "**Bayes Classifier**\n",
      "* Test error rate is minimized on average by classifer which assigns each observation to most likely class, given its predictor values\n",
      "\n",
      "$Pr(Y = j\\mid X = x_0)$\n",
      "\n",
      "* Conditional Probability: probability that Y = j, given observed predictor vector of $x_0$\n",
      "* This simple classifier is called Bayes classifier - it choses the class which the probability is the largest\n",
      "* So the error rate will be the 1 - probability\n",
      "\n",
      "$1 - E(\\max\\limits_j Pr(Y = j\\mid X))$\n",
      "\n",
      "* Bayes error rate = 1 - expected max probability, given all values of X\n",
      "\n",
      "* Bayes decision boundary = present the points where probability is 50%\n",
      "* It produces lowest possible test error rate (Bayes error rate) - irreducible error\n",
      "\n",
      "**K-Nearest Neighbors**\n",
      "* In real data, we do not know the conditional distribution of Y given X, so Bayes classifier is impossible, but it's used as gold standard to compare other methods\n",
      "* kNN - estimate the conditional distribution of Y given X, then classify each observation to the class with highest *estimated* probability.\n",
      "\n",
      "$Pr(Y = j \\mid X = x_0) = \\frac{1}{K} \\sum I(y_i = j)$\n",
      "\n",
      "* KNN identifies the closest K points to $x_0$, estimates the probability as fraction of points, then classify it based on highest fraction\n",
      "* if K = 3, KNN will choose the first 3 observations closest to the point, estimate it's probability (2/3) and classify it.\n",
      "* Choice of k is very important, k = 1 (highly flexibie)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}